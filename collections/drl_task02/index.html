<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Bellman Optimal Equation 回顾上次学的 Bellman Equation，它到底在做什么事情呢？我们先定义了 state value 和 action value，他们为什么重要，就是因为评价一个策略的好坏需要量化的指标，value 就是这样的一个指标。现在寻找最优策略的目标等价于寻找最大的 value 值。Bellman Equation 做的事就是用三个基本概率（策略、回报和状态），来表示 state value 或 action value。
$$ \pi(a|s),P(r|s,a),P(s&amp;rsquo;|s,a)\Rightarrow V(s),Q(s,a) $$
Bellman Equation 的矩阵形式是
$$ v_{\pi}=r+\gamma P_{\pi}v_{\pi} $$
我们的目标是首先最大化 $v_{\pi}$，然后找到对应的策略 $\pi$。所以我们面临的是这样一个最优化问题
$$ v_{\pi}=\max_{\pi} (r+\gamma P_{\pi}v_{\pi}) $$
其中 $r,P$ 都是模型中已知的，两个未知数为 $\pi,v$
先不管左边，单看右边就是个基本的最优化问题，此时变量是 $\max$ 下面的东西，其他都看作常数。我们把右边等式设做 $f$
$$ f(x)=\max_y(x^3-xy^2+y+3) $$
为什么 $f(\cdot)$ 里只有 $x$ 没有 $y$ 呢？等我们做完优化后就显而易见了。
$$ \frac{\partial }{\partial y}(x^3-xy^2+y+3)=-2xy+1=0 $$
解得 $y=1/(2x)$，这是右边式子的最优解，当然严谨起见应该求 $f$ 的二阶导，验证 $f&amp;rsquo;&amp;rsquo;&amp;lt;0$。Anyway，回代 $y$ 值，得到"><title>task02 - BOE，三种迭代</title>
<link rel=canonical href=https://markdafu.github.io/collections/drl_task02/><link rel=stylesheet href=/scss/style.min.aca18fe454c5ea0a109b52860800655e14184bb040f8c3f6c9a477e2d6ac2d74.css><meta property="og:title" content="task02 - BOE，三种迭代"><meta property="og:description" content="Bellman Optimal Equation 回顾上次学的 Bellman Equation，它到底在做什么事情呢？我们先定义了 state value 和 action value，他们为什么重要，就是因为评价一个策略的好坏需要量化的指标，value 就是这样的一个指标。现在寻找最优策略的目标等价于寻找最大的 value 值。Bellman Equation 做的事就是用三个基本概率（策略、回报和状态），来表示 state value 或 action value。
$$ \pi(a|s),P(r|s,a),P(s&amp;rsquo;|s,a)\Rightarrow V(s),Q(s,a) $$
Bellman Equation 的矩阵形式是
$$ v_{\pi}=r+\gamma P_{\pi}v_{\pi} $$
我们的目标是首先最大化 $v_{\pi}$，然后找到对应的策略 $\pi$。所以我们面临的是这样一个最优化问题
$$ v_{\pi}=\max_{\pi} (r+\gamma P_{\pi}v_{\pi}) $$
其中 $r,P$ 都是模型中已知的，两个未知数为 $\pi,v$
先不管左边，单看右边就是个基本的最优化问题，此时变量是 $\max$ 下面的东西，其他都看作常数。我们把右边等式设做 $f$
$$ f(x)=\max_y(x^3-xy^2+y+3) $$
为什么 $f(\cdot)$ 里只有 $x$ 没有 $y$ 呢？等我们做完优化后就显而易见了。
$$ \frac{\partial }{\partial y}(x^3-xy^2+y+3)=-2xy+1=0 $$
解得 $y=1/(2x)$，这是右边式子的最优解，当然严谨起见应该求 $f$ 的二阶导，验证 $f&amp;rsquo;&amp;rsquo;&amp;lt;0$。Anyway，回代 $y$ 值，得到"><meta property="og:url" content="https://markdafu.github.io/collections/drl_task02/"><meta property="og:site_name" content="Diaspora"><meta property="og:type" content="article"><meta property="article:section" content="Collections"><meta property="article:published_time" content="2023-11-18T00:00:00+00:00"><meta property="article:modified_time" content="2023-11-18T00:00:00+00:00"><meta name=twitter:title content="task02 - BOE，三种迭代"><meta name=twitter:description content="Bellman Optimal Equation 回顾上次学的 Bellman Equation，它到底在做什么事情呢？我们先定义了 state value 和 action value，他们为什么重要，就是因为评价一个策略的好坏需要量化的指标，value 就是这样的一个指标。现在寻找最优策略的目标等价于寻找最大的 value 值。Bellman Equation 做的事就是用三个基本概率（策略、回报和状态），来表示 state value 或 action value。
$$ \pi(a|s),P(r|s,a),P(s&amp;rsquo;|s,a)\Rightarrow V(s),Q(s,a) $$
Bellman Equation 的矩阵形式是
$$ v_{\pi}=r+\gamma P_{\pi}v_{\pi} $$
我们的目标是首先最大化 $v_{\pi}$，然后找到对应的策略 $\pi$。所以我们面临的是这样一个最优化问题
$$ v_{\pi}=\max_{\pi} (r+\gamma P_{\pi}v_{\pi}) $$
其中 $r,P$ 都是模型中已知的，两个未知数为 $\pi,v$
先不管左边，单看右边就是个基本的最优化问题，此时变量是 $\max$ 下面的东西，其他都看作常数。我们把右边等式设做 $f$
$$ f(x)=\max_y(x^3-xy^2+y+3) $$
为什么 $f(\cdot)$ 里只有 $x$ 没有 $y$ 呢？等我们做完优化后就显而易见了。
$$ \frac{\partial }{\partial y}(x^3-xy^2+y+3)=-2xy+1=0 $$
解得 $y=1/(2x)$，这是右边式子的最优解，当然严谨起见应该求 $f$ 的二阶导，验证 $f&amp;rsquo;&amp;rsquo;&amp;lt;0$。Anyway，回代 $y$ 值，得到"><link rel="shortcut icon" href=favicon.png></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label="Toggle Menu">
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/avatar_hu2991a6378885d4c497029adaf7f4f579_278431_300x0_resize_box_3.png width=300 height=300 class=site-logo loading=lazy alt=Avatar>
</a><span class=emoji>🔮</span></figure><div class=site-meta><h1 class=site-name><a href=/>Diaspora</a></h1><h2 class=site-description>Quid Tum?</h2></div></header><ol class=social-menu><li><a href=https://github.com/markdafu/repodemo target=_blank title=GitHub rel=me><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li><li><a href=https://twitter.com target=_blank title=Twitter rel=me><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-twitter" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M22 4.01c-1 .49-1.98.689-3 .99-1.121-1.265-2.783-1.335-4.38-.737S11.977 6.323 12 8v1c-3.245.083-6.135-1.395-8-4 0 0-4.182 7.433 4 11-1.872 1.247-3.739 2.088-6 2 3.308 1.803 6.913 2.423 10.034 1.517 3.58-1.04 6.522-3.723 7.651-7.742a13.84 13.84.0 00.497-3.753C20.18 7.773 21.692 5.25 22 4.009z"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg><span>Home</span></a></li><li><a href=/archives/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg><span>Archives</span></a></li><li><a href=/search/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg><span>Search</span></a></li><li><a href=/links/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M10 14a3.5 3.5.0 005 0l4-4a3.5 3.5.0 00-5-5l-.5.5"/><path d="M14 10a3.5 3.5.0 00-5 0l-4 4a3.5 3.5.0 005 5l.5-.5"/></svg><span>Links</span></a></li><div class=menu-bottom-section><li id=dark-mode-toggle><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg><span>Dark Mode</span></li></div></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">Table of contents</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#bellman-optimal-equation>Bellman Optimal Equation</a></li><li><a href=#三种迭代方法>三种迭代方法</a><ol><li><a href=#value-iteration>Value Iteration</a></li><li><a href=#policy-iteration>Policy Iteration</a></li><li><a href=#truncated-policy-iteration>Truncated Policy Iteration</a></li></ol></li></ol></nav></div></section></aside><main class="main full-width"><article class=main-article><header class=article-header><div class=article-details><div class=article-title-wrapper><h2 class=article-title><a href=/collections/drl_task02/>task02 - BOE，三种迭代</a></h2></div><footer class=article-time><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg><time class=article-time--published>Nov 18, 2023</time></div><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg><time class=article-time--reading>1 minute read</time></div></footer></div></header><section class=article-content><h2 id=bellman-optimal-equation>Bellman Optimal Equation</h2><p>回顾上次学的 Bellman Equation，它到底在做什么事情呢？我们先定义了 state value 和 action value，他们为什么重要，就是因为评价一个策略的好坏需要量化的指标，value 就是这样的一个指标。现在寻找最优策略的目标等价于寻找最大的 value 值。Bellman Equation 做的事就是用三个基本概率（策略、回报和状态），来表示 state value 或 action value。</p><p>$$
\pi(a|s),P(r|s,a),P(s&rsquo;|s,a)\Rightarrow V(s),Q(s,a)
$$</p><p>Bellman Equation 的矩阵形式是</p><p>$$
v_{\pi}=r+\gamma P_{\pi}v_{\pi}
$$</p><p>我们的目标是首先最大化 $v_{\pi}$，然后找到对应的策略 $\pi$。所以我们面临的是这样一个最优化问题</p><p>$$
v_{\pi}=\max_{\pi} (r+\gamma P_{\pi}v_{\pi})
$$</p><p>其中 $r,P$ 都是模型中已知的，两个未知数为 $\pi,v$</p><p>先不管左边，单看右边就是个基本的最优化问题，此时变量是 $\max$ 下面的东西，其他都看作常数。我们把右边等式设做 $f$</p><p>$$
f(x)=\max_y(x^3-xy^2+y+3)
$$</p><p>为什么 $f(\cdot)$ 里只有 $x$ 没有 $y$ 呢？等我们做完优化后就显而易见了。</p><p>$$
\frac{\partial }{\partial y}(x^3-xy^2+y+3)=-2xy+1=0
$$</p><p>解得 $y=1/(2x)$，这是右边式子的最优解，当然严谨起见应该求 $f$ 的二阶导，验证 $f&rsquo;&rsquo;&lt;0$。Anyway，回代 $y$ 值，得到</p><p>$$
f(x)=x^3+\frac{1}{4x}+3
$$</p><p>优化后 $f$ 可以完全表示成 $x$ 的函数</p><p>回到贝尔曼最优方程，右式优化后，$\pi^*=\pi(v)$，整个右边可以表示成 $v$ 的函数 $f(v)$。</p><p>$$
v=f(v)
$$</p><p>由<a class=link href=https://zh.wikipedia.org/zh-hans/%E5%B7%B4%E6%8B%BF%E8%B5%AB%E4%B8%8D%E5%8A%A8%E7%82%B9%E5%AE%9A%E7%90%86 target=_blank rel=noopener>不懂点原理</a>，$v=f(v)$ 有唯一解，且迭代 $v_{k+1} = f(v_k)$ 会收敛。</p><p>接下来就介绍迭代求解 $v,\pi$ 的三种方法</p><h2 id=三种迭代方法>三种迭代方法</h2><ol><li>Value iteration</li><li>Policy iteration</li><li>Truncated policy iteration</li></ol><p>前两种迭代是第三种的基本型</p><h3 id=value-iteration>Value Iteration</h3><p>值迭代分为两步：policy update (PU) 和 value update (VU)。</p><p>给定初始化的 $v_0$</p><p>Policy update:</p><p>$$
\pi_1 = \argmax_{\pi}(r_{\pi}+\gamma P_{\pi}v_0)
$$</p><p>Value update:</p><p>$$
v_1=r_{\pi_1}+\gamma P_{\pi_{1}}v_0
$$</p><p>得到 $v_1$ 后，再用于下一次迭代</p><h3 id=policy-iteration>Policy Iteration</h3><p>策略迭代也分为两步：policy evaluation (PE) 和 policy improvement (PI)</p><p>给定初始化的策略 $\pi_0$</p><p>Policy evaluation:</p><p>$$
v_{\pi_0}=r_{\pi_0}+\gamma P_{\pi_0}v_{\pi_0}
$$</p><p>把 $\pi_0$ 代入贝尔曼公式，用迭代求解 $v_{\pi_0}$（大迭代里的小迭代）</p><blockquote><p>$$v_{\pi_0}^{(j+1)}=r_{\pi_0}+\gamma P_{\pi_0}v_{\pi_0}^{(j)}$$</p></blockquote><p>Policy improvement:</p><p>$$
\pi_1 = \argmax_{\pi}(r_{\pi}+\gamma P_{\pi}v_{\pi_0})
$$</p><p><img src=/collections/drl_task02/image-1.png width=1362 height=220 srcset="/collections/drl_task02/image-1_hu66ecd789533a2091c29c022ab0a1500e_44753_480x0_resize_box_3.png 480w, /collections/drl_task02/image-1_hu66ecd789533a2091c29c022ab0a1500e_44753_1024x0_resize_box_3.png 1024w" loading=lazy alt="Two Iterations" class=gallery-image data-flex-grow=619 data-flex-basis=1485px></p><h3 id=truncated-policy-iteration>Truncated Policy Iteration</h3><p><img src=/collections/drl_task02/image-3.png width=1740 height=745 srcset="/collections/drl_task02/image-3_hu620731d65879060069eb515da1395150_160859_480x0_resize_box_3.png 480w, /collections/drl_task02/image-3_hu620731d65879060069eb515da1395150_160859_1024x0_resize_box_3.png 1024w" loading=lazy alt=Comparison class=gallery-image data-flex-grow=233 data-flex-basis=560px></p><p>前三步是等价的，第四步开始有不同（Policy iteration 中小迭代的部分）：Policy 迭代直接把 $v_{\pi_1}$ 求了出来， Value 迭代则只往前迭代一步</p><p><img src=/collections/drl_task02/image-2.png width=2009 height=959 srcset="/collections/drl_task02/image-2_huf8873dda9a85eb0367a8e11423c3f73b_129007_480x0_resize_box_3.png 480w, /collections/drl_task02/image-2_huf8873dda9a85eb0367a8e11423c3f73b_129007_1024x0_resize_box_3.png 1024w" loading=lazy alt=Relationship class=gallery-image data-flex-grow=209 data-flex-basis=502px></p></section><footer class=article-footer><section class=article-copyright><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg><span></span></section></footer><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.6/dist/katex.min.css integrity="sha256-J+iAE0sgH8QSz9hpcDxXIftnj65JEZgNhGcgReTTK9s=" crossorigin=anonymous><script src=https://cdn.jsdelivr.net/npm/katex@0.15.6/dist/katex.min.js integrity="sha256-InsNdER1b2xUewP+pKCUJpkhiqwHgqiPXDlIk7GzBu4=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/katex@0.15.6/dist/contrib/auto-render.min.js integrity="sha256-y39Mpg7V3D4lhBX4x6O0bUqTV4pSrfgwEfGKfxkOdgI=" crossorigin=anonymous defer></script><script>window.addEventListener("DOMContentLoaded",()=>{renderMathInElement(document.querySelector(`.article-content`),{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],ignoredClasses:["gist"]})})</script></article><div class=disqus-container><div id=disqus_thread></div><script>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//hugo-theme-stack.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></div><style>.disqus-container{background-color:var(--card-background);border-radius:var(--card-border-radius);box-shadow:var(--shadow-l1);padding:var(--card-padding)}</style><script>window.addEventListener("onColorSchemeChange",e=>{typeof DISQUS=="object"&&DISQUS.reset({reload:!0})})</script><footer class=site-footer><section class=copyright>&copy;
2023 -
2024 Diaspora</section><section class=powerby>Mark's personal repository<br>Built with <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a><br>Theme <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.21.0>Stack</a></b> designed by <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a></section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>