<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Markov Decision Process Markov过程表现以概率动态变化的结构，是很简化的模型，也是强化学习的基础。Markov性质指下一步的状态只和这一步有关，没有记忆。
注意这里的顺序，时刻$t$，agent 在状态$s_t$和奖励$r_t$的环境下，做出$a_t$的行为。
MDP 作为强化学习的豌豆射手，就是叠加奖励和动作的Markov过程
$$ \text{Markov}过程\xrightarrow{+奖励}\text{Markov}奖励过程 (MRP)\xrightarrow{+动作}\text{Markov}决策过程 (MDP) $$
动态规划 一道面试题 $$ f(i,j)=\begin{cases} 0 &amp;amp; i=0,j=0\\ 1 &amp;amp; i=0,j\neq 0\\ 1 &amp;amp; i\neq 0,j=0\\ f(i-1,j) + f(i,j-1) &amp;amp; i\neq 0,j\neq 0 \end{cases} $$
分类讨论是要留意贴边的特殊情况，在编码中也要初始化边界
1 2 3 4 5 6 7 8 def solve(m,n): # 初始化边界条件 f = [[1] * n] + [[1] + [0] * (n - 1) for _ in range(m - 1)] # 状态转移 for i in range(1, m): for j in range(1, n): f[i][j] = f[i - 1][j] + f[i][j - 1] return f[m - 1][n - 1] 这里 f = [[1] * n] + [[1] + [0] * (n - 1) for _ in range(m - 1)] 是比较 pythonic 的写法，假设$m=4,n=3$，发现"><title>task01 - MDP，动态规划</title>
<link rel=canonical href=https://markdafu.github.io/collections/drl_task01/><link rel=stylesheet href=/scss/style.min.aca18fe454c5ea0a109b52860800655e14184bb040f8c3f6c9a477e2d6ac2d74.css><meta property="og:title" content="task01 - MDP，动态规划"><meta property="og:description" content="Markov Decision Process Markov过程表现以概率动态变化的结构，是很简化的模型，也是强化学习的基础。Markov性质指下一步的状态只和这一步有关，没有记忆。
注意这里的顺序，时刻$t$，agent 在状态$s_t$和奖励$r_t$的环境下，做出$a_t$的行为。
MDP 作为强化学习的豌豆射手，就是叠加奖励和动作的Markov过程
$$ \text{Markov}过程\xrightarrow{+奖励}\text{Markov}奖励过程 (MRP)\xrightarrow{+动作}\text{Markov}决策过程 (MDP) $$
动态规划 一道面试题 $$ f(i,j)=\begin{cases} 0 &amp;amp; i=0,j=0\\ 1 &amp;amp; i=0,j\neq 0\\ 1 &amp;amp; i\neq 0,j=0\\ f(i-1,j) + f(i,j-1) &amp;amp; i\neq 0,j\neq 0 \end{cases} $$
分类讨论是要留意贴边的特殊情况，在编码中也要初始化边界
1 2 3 4 5 6 7 8 def solve(m,n): # 初始化边界条件 f = [[1] * n] + [[1] + [0] * (n - 1) for _ in range(m - 1)] # 状态转移 for i in range(1, m): for j in range(1, n): f[i][j] = f[i - 1][j] + f[i][j - 1] return f[m - 1][n - 1] 这里 f = [[1] * n] + [[1] + [0] * (n - 1) for _ in range(m - 1)] 是比较 pythonic 的写法，假设$m=4,n=3$，发现"><meta property="og:url" content="https://markdafu.github.io/collections/drl_task01/"><meta property="og:site_name" content="Diaspora"><meta property="og:type" content="article"><meta property="article:section" content="Collections"><meta property="article:published_time" content="2023-11-13T00:00:00+00:00"><meta property="article:modified_time" content="2023-11-13T00:00:00+00:00"><meta name=twitter:title content="task01 - MDP，动态规划"><meta name=twitter:description content="Markov Decision Process Markov过程表现以概率动态变化的结构，是很简化的模型，也是强化学习的基础。Markov性质指下一步的状态只和这一步有关，没有记忆。
注意这里的顺序，时刻$t$，agent 在状态$s_t$和奖励$r_t$的环境下，做出$a_t$的行为。
MDP 作为强化学习的豌豆射手，就是叠加奖励和动作的Markov过程
$$ \text{Markov}过程\xrightarrow{+奖励}\text{Markov}奖励过程 (MRP)\xrightarrow{+动作}\text{Markov}决策过程 (MDP) $$
动态规划 一道面试题 $$ f(i,j)=\begin{cases} 0 &amp;amp; i=0,j=0\\ 1 &amp;amp; i=0,j\neq 0\\ 1 &amp;amp; i\neq 0,j=0\\ f(i-1,j) + f(i,j-1) &amp;amp; i\neq 0,j\neq 0 \end{cases} $$
分类讨论是要留意贴边的特殊情况，在编码中也要初始化边界
1 2 3 4 5 6 7 8 def solve(m,n): # 初始化边界条件 f = [[1] * n] + [[1] + [0] * (n - 1) for _ in range(m - 1)] # 状态转移 for i in range(1, m): for j in range(1, n): f[i][j] = f[i - 1][j] + f[i][j - 1] return f[m - 1][n - 1] 这里 f = [[1] * n] + [[1] + [0] * (n - 1) for _ in range(m - 1)] 是比较 pythonic 的写法，假设$m=4,n=3$，发现"><link rel="shortcut icon" href=favicon.png></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label="Toggle Menu">
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/avatar_hu2991a6378885d4c497029adaf7f4f579_278431_300x0_resize_box_3.png width=300 height=300 class=site-logo loading=lazy alt=Avatar>
</a><span class=emoji>🔮</span></figure><div class=site-meta><h1 class=site-name><a href=/>Diaspora</a></h1><h2 class=site-description>Quid Tum?</h2></div></header><ol class=social-menu><li><a href=https://github.com/markdafu/repodemo target=_blank title=GitHub rel=me><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li><li><a href=https://twitter.com target=_blank title=Twitter rel=me><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-twitter" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M22 4.01c-1 .49-1.98.689-3 .99-1.121-1.265-2.783-1.335-4.38-.737S11.977 6.323 12 8v1c-3.245.083-6.135-1.395-8-4 0 0-4.182 7.433 4 11-1.872 1.247-3.739 2.088-6 2 3.308 1.803 6.913 2.423 10.034 1.517 3.58-1.04 6.522-3.723 7.651-7.742a13.84 13.84.0 00.497-3.753C20.18 7.773 21.692 5.25 22 4.009z"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg><span>Home</span></a></li><li><a href=/archives/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg><span>Archives</span></a></li><li><a href=/search/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg><span>Search</span></a></li><li><a href=/links/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M10 14a3.5 3.5.0 005 0l4-4a3.5 3.5.0 00-5-5l-.5.5"/><path d="M14 10a3.5 3.5.0 00-5 0l-4 4a3.5 3.5.0 005 5l.5-.5"/></svg><span>Links</span></a></li><div class=menu-bottom-section><li id=dark-mode-toggle><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg><span>Dark Mode</span></li></div></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">Table of contents</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#markov-decision-process>Markov Decision Process</a></li><li><a href=#动态规划>动态规划</a><ol><li><a href=#一道面试题>一道面试题</a></li><li><a href=#rl中的动态规划>RL中的动态规划</a></li></ol></li><li><a href=#bellman-equation>Bellman Equation</a><ol><li><a href=#公式推导>公式推导</a></li><li><a href=#求解-bellman-equation>求解 Bellman Equation</a></li></ol></li><li><a href=#附joyrl-typo>附：JoyRL typo</a></li></ol></nav></div></section></aside><main class="main full-width"><article class=main-article><header class=article-header><div class=article-details><div class=article-title-wrapper><h2 class=article-title><a href=/collections/drl_task01/>task01 - MDP，动态规划</a></h2></div><footer class=article-time><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg><time class=article-time--published>Nov 13, 2023</time></div><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg><time class=article-time--reading>3 minute read</time></div></footer></div></header><section class=article-content><h2 id=markov-decision-process>Markov Decision Process</h2><p>Markov过程表现以概率动态变化的结构，是很简化的模型，也是强化学习的基础。Markov性质指下一步的状态只和这一步有关，没有记忆。</p><center><img src=image-4.png width=500px></center><p>注意这里的顺序，时刻$t$，agent 在状态$s_t$和奖励$r_t$的环境下，做出$a_t$的行为。</p><p>MDP 作为强化学习的豌豆射手，就是叠加奖励和动作的Markov过程</p><p>$$
\text{Markov}过程\xrightarrow{+奖励}\text{Markov}奖励过程 (MRP)\xrightarrow{+动作}\text{Markov}决策过程 (MDP)
$$</p><h2 id=动态规划>动态规划</h2><h3 id=一道面试题>一道面试题</h3><center><img src=image-5.png class=center width=500px></center><p>$$
f(i,j)=\begin{cases}
0 & i=0,j=0\\
1 & i=0,j\neq 0\\
1 & i\neq 0,j=0\\
f(i-1,j) + f(i,j-1) & i\neq 0,j\neq 0
\end{cases}
$$</p><p>分类讨论是要留意贴边的特殊情况，在编码中也要初始化边界</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>solve</span><span class=p>(</span><span class=n>m</span><span class=p>,</span><span class=n>n</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=c1># 初始化边界条件</span>
</span></span><span class=line><span class=cl>    <span class=n>f</span> <span class=o>=</span> <span class=p>[[</span><span class=mi>1</span><span class=p>]</span> <span class=o>*</span> <span class=n>n</span><span class=p>]</span> <span class=o>+</span> <span class=p>[[</span><span class=mi>1</span><span class=p>]</span> <span class=o>+</span> <span class=p>[</span><span class=mi>0</span><span class=p>]</span> <span class=o>*</span> <span class=p>(</span><span class=n>n</span> <span class=o>-</span> <span class=mi>1</span><span class=p>)</span> <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>m</span> <span class=o>-</span> <span class=mi>1</span><span class=p>)]</span> 
</span></span><span class=line><span class=cl>    <span class=c1># 状态转移</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=n>m</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>j</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=n>n</span><span class=p>):</span>
</span></span><span class=line><span class=cl>            <span class=n>f</span><span class=p>[</span><span class=n>i</span><span class=p>][</span><span class=n>j</span><span class=p>]</span> <span class=o>=</span> <span class=n>f</span><span class=p>[</span><span class=n>i</span> <span class=o>-</span> <span class=mi>1</span><span class=p>][</span><span class=n>j</span><span class=p>]</span> <span class=o>+</span> <span class=n>f</span><span class=p>[</span><span class=n>i</span><span class=p>][</span><span class=n>j</span> <span class=o>-</span> <span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>f</span><span class=p>[</span><span class=n>m</span> <span class=o>-</span> <span class=mi>1</span><span class=p>][</span><span class=n>n</span> <span class=o>-</span> <span class=mi>1</span><span class=p>]</span>
</span></span></code></pre></td></tr></table></div></div><p>这里 <code>f = [[1] * n] + [[1] + [0] * (n - 1) for _ in range(m - 1)]</code> 是比较 pythonic 的写法，假设$m=4,n=3$，发现</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>f = [[1, 1, 1], [1, 0, 0], [1, 0, 0], [1, 0, 0]]
</span></span></code></pre></td></tr></table></div></div><p>$$
f=\begin{bmatrix}
1 & 1 & 1 & 1\\
1 & 0 & 0 & 0\\
1 & 0 & 0 & 0
\end{bmatrix}
$$</p><h3 id=rl中的动态规划>RL中的动态规划</h3><p>首先规范 Notation 很重要。基本型是 $S_t\xrightarrow{A_t}R_{t+1},S_{t+1}$，其中 $R_{t+1}$ 表示在状态 $S_t$ 时做 $A_t$ 的动作后收获的回报（这里用 $t+1$ 而不是 $t$ 纯粹是 convention，大部分 manual 上都这样写）。这个模型中有三个重要组成部分：策略、回报和状态价值（state value），都写成条件概率的形式</p><div class=table-wrapper><table><thead><tr><th style=text-align:center>图</th><th style=text-align:center>名字</th><th style=text-align:center>表达式</th></tr></thead><tbody><tr><td style=text-align:center>$S_t\rightarrow A_t$</td><td style=text-align:center>策略</td><td style=text-align:center>$\pi(A_t=a\vert S_t=s)$</td></tr><tr><td style=text-align:center>$S_t,A_t\rightarrow R_{t+1}$</td><td style=text-align:center>回报</td><td style=text-align:center>$P(R_{t+1}=r\vert S_t=s,A_t=a)$</td></tr><tr><td style=text-align:center>$S_t,A_t\rightarrow S_{t+1}$</td><td style=text-align:center>state probability</td><td style=text-align:center>$P(S_{t+1}=s&rsquo;\vert S_t=s,A_t=a)$</td></tr></tbody></table></div><p>扩展到多期，图为</p><p>$$
S_t\xrightarrow{A_t}R_{t+1},S_{t+1}\xrightarrow{A_{t+1}}R_{t+2},S_{t+2}\xrightarrow{A_{t+2}}R_{t+3},\cdots
$$</p><p>上面的这一串行为和状态我们称作一个 trajectory。它的价值是所有的回报折现，这和金融和会计里现金流的折现是一样的，很好理解。</p><p>$$
G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3}+\cdots
$$</p><h2 id=bellman-equation>Bellman Equation</h2><p>首先定义 state value。对于一个状态集 $\mathcal{S}$，任取一个状态 $\forall s\in \mathcal{S}$，当 $t$ 时刻的状态 $S_t=s$ 时，由此展开的所有可能 trajectory 的回报（$G_t$）的期望</p><p>$$
V_{\pi}(s)=E[G_t\vert S_t=s]
$$</p><p>这个 $V_{\pi}(s)$ 就是 state value 的表达式。很直观地，它和当前状态 $s$ 以及策略 $\pi$ 有关，因此是基于 $s,\pi$ 的函数。</p><h3 id=公式推导>公式推导</h3><p>现在有意思了。我们把 $G_t$ 换种形式写</p><p>$$
G_t=R_{t+1}+\gamma G_{t+1}
$$</p><p>整个 Bellman Equation 的推导都由上式展开</p><p>$$
\begin{aligned}
V_{\pi}(s)&=E[G_t\vert S_t=s]\\
&=E[R_{t+1}+\gamma G_{t+1}\vert S_t=s]\\
&=E[R_{t+1}\vert S_t=s]+\gamma E[G_{t+1}\vert S_t=s] \tag{$\star$}
\end{aligned}
$$</p><p>拆开后，也是很 intuitive 的看到，第一项是当期回报的期望，第二项是未来无限期回报的总期望</p><p><strong>第一项</strong></p><p>$$
E[R_{t+1}\vert S_t=s]=\sum_{i=1}^n\pi(a_i|s)\cdot E[R_{t+1}|S_t=s,A_t=a_i]
$$</p><p>$\pi(a_i|s)$ 是给定当前状态 $S_t=s$ 的情况下，做出行为 $a_i$ 的概率。很眼熟，这是 <a class=link href=https://en.wikipedia.org/wiki/Law_of_total_expectation target=_blank rel=noopener>law of iterated expextation (LIE)</a>，可以把上式改写成</p><p>$$
E[R_{t+1}\vert S_t=s]=E\bigg[E(R_{t+1}\vert S_t=s,A_t=a_i)\bigg]
$$</p><p>其中</p><p>$$
E(R_{t+1}\vert S_t=s,A_t=a_i)=\sum_r P(R_{t+1}=r|S_t=s,A_t=a_i)r
$$</p><p>注：之后像这种 $P(R_{t+1}=r|S_t=s,A_t=a_i)$ 就简写成 $P(r|s,a_i)$</p><p><strong>第二项</strong></p><p>$$
E[G_{t+1}\vert S_t=s]=\sum_{s&rsquo;}E[G_{t+1}|S_t=s,S_{t+1}=s&rsquo;]P(S_{t+1}=s&rsquo;|S_t=s)
$$</p><p>因为 $G_{t+1}$ 跟上一期状态 $S_t$ 没关系（Markov性质），故</p><p>$$
E[G_{t+1}\vert S_t=s]=\sum_{s&rsquo;}E[G_{t+1}|S_{t+1}=s&rsquo;]P(S_{t+1}=s&rsquo;|S_t=s)=\sum_{s&rsquo;}V_{\pi}(s&rsquo;)P(s&rsquo;|s)
$$</p><p>将 $P(s&rsquo;|s)$ 依行为 $a_i$ 展开为 $\sum_iP(s&rsquo;|s,a_i)\cdot \pi(a_i|s)$，整理</p><p>$$
\begin{aligned}
\sum_{s&rsquo;}V_{\pi}(s&rsquo;)P(s&rsquo;|s)&=\sum_{s&rsquo;}V_{\pi}(s&rsquo;)\sum_iP(s&rsquo;|s,a_i)\cdot \pi(a_i|s)\\
&=\sum_i \left[\sum_{s&rsquo;}V_{\pi}(s&rsquo;)\right] P(s&rsquo;|s,a_i)\cdot \pi(a_i|s)\\
&=\sum_i \pi(a_i|s)\cdot \left[\sum_{s&rsquo;}V_{\pi}(s&rsquo;) \cdot P(s&rsquo;|s,a_i)\right]
\end{aligned}
$$</p><p>把处理后的两项代回 $(\star)$ 式，</p><p>$$
V_{\pi}(s)=\sum_i \pi(a_i|s)\cdot \left[ \sum_r P(r|s,a_i)\cdot r+\gamma\sum_{s&rsquo;}V_{\pi}(s&rsquo;) \cdot P(s&rsquo;|s,a_i)\right]
$$</p><p>注意这不是一个式子，而是对所有的 $s\in \mathcal{S}$ 都成立</p><p>这就是 state value 的 Bellman Equation，其实还有 action value 的，之后再说</p><p><strong>更新：Action value 的贝尔曼方程</strong></p><p>$$
E[G_t|S_t=s]=\sum_i \pi(a_i|s) E[G_t|S_t=s,A_t=a_i]
$$</p><p>其中 $E[G_t|S_t=s,A_t=a_i]=Q_{\pi}(s,a_i)$，也就是在策略 $\pi$ 下，给定状态和行动 $s,a$ 产生的价值。因为相比 $V_{\pi}(s)$ 纳入了行动 $a$，故称为 action value。因此上式也可写作</p><p>$$
V_{\pi}(s)=\sum_i \pi(a_i|s)\cdot Q_{\pi}(s,a_i) \tag{1}
$$</p><p>对比之前 state value 的贝尔曼方程</p><p>$$
V_{\pi}(s)=\sum_i \pi(a_i|s)\cdot \left[ \sum_r P(r|s,a_i)\cdot r+\gamma\sum_{s&rsquo;}V_{\pi}(s&rsquo;) \cdot P(s&rsquo;|s,a_i)\right]
$$</p><p>容易发现</p><p>$$
Q_{\pi}(s,a_i)=\sum_r P(r|s,a_i)\cdot r+\gamma\sum_{s&rsquo;}V_{\pi}(s&rsquo;) \cdot P(s&rsquo;|s,a_i) \tag{2}
$$</p><p>式 $(1)$ 是由 action value 推 state value，式 $(2)$ 是由 state value 推 action value</p><h3 id=求解-bellman-equation>求解 Bellman Equation</h3><p>推广为矩阵形式</p><p>$$
G_t=R_{t+1}+\gamma G_{t+1}\Rightarrow \vec{v_{\pi}}=\vec{r_{\pi}}+\gamma P_{\pi}\vec{v_{\pi}}
$$</p><p>其中 $P_{\pi}$ 是状态转移矩阵</p><p>$[解一]$ 矩阵求逆</p><p>$$
v_{\pi} = (I-\gamma P_{\pi})^{-1}r_{\pi}
$$</p><p>当状态很多时矩阵会非常大，求逆难度很高</p><p>$[解二]$ 数值解（迭代）</p><p>$$
v_{k+1}=r_{\pi}+\gamma P_{\pi}v_{k}
$$</p><p>可以证明 $\vec{v_{k}}\rightarrow \vec{v_{\pi}}, (k\rightarrow \infty)$</p><p>思路：设 $\delta_k=v_k-v_{\pi}$，用 $\delta$ 表示 $v_k,v_{k+1}$，再代入 $v_{k+1}=r_{\pi}+\gamma P_{\pi}v_{k}$，化简会发现 $\delta_{k+1}=\gamma P_{\pi}\delta_k$，所以 $\delta_{k+n}=(\gamma P_{\pi})^n\delta_k$。因为 $\gamma&lt;1$，状态转移矩阵的范数 $\Vert P_{\pi}\Vert \leqslant 1$（概率不能超过 1），所以 $(\gamma P_{\pi})^n\rightarrow 0,n\rightarrow \infty$，因此 $\delta_k\rightarrow 0$. $\square$</p><blockquote><p>参考：赵世钰</p></blockquote><div class=video-wrapper><iframe src="https://player.bilibili.com/player.html?as_wide=1&amp;high_quality=1&amp;page=9&bvid=BV1sd4y167NS" scrolling=no frameborder=no framespacing=0 allowfullscreen></iframe></div><h2 id=附joyrl-typo>附：JoyRL typo</h2><p><strong>2.3</strong></p><p><img src=/collections/drl_task01/image.png width=1296 height=74 srcset="/collections/drl_task01/image_hu4a7181f7e28e79d9f64db6edf975fa00_27988_480x0_resize_box_3.png 480w, /collections/drl_task01/image_hu4a7181f7e28e79d9f64db6edf975fa00_27988_1024x0_resize_box_3.png 1024w" loading=lazy alt=typo1 class=gallery-image data-flex-grow=1751 data-flex-basis=4203px></p><p>将贝尔曼公式 $\Rightarrow$ “讲”贝尔曼公式</p><p><strong>2.4</strong></p><p><img src=/collections/drl_task01/image-1.png width=1604 height=114 srcset="/collections/drl_task01/image-1_hu783b405959ae87ef15f2a50c84587d8e_57904_480x0_resize_box_3.png 480w, /collections/drl_task01/image-1_hu783b405959ae87ef15f2a50c84587d8e_57904_1024x0_resize_box_3.png 1024w" loading=lazy alt=typo2 class=gallery-image data-flex-grow=1407 data-flex-basis=3376px></p><p>景止环境 $\Rightarrow$ 静止环境</p><p><img src=/collections/drl_task01/image-2.png width=1610 height=160 srcset="/collections/drl_task01/image-2_hu18db97e1f9ca64c888d0247c3e33ce02_83504_480x0_resize_box_3.png 480w, /collections/drl_task01/image-2_hu18db97e1f9ca64c888d0247c3e33ce02_83504_1024x0_resize_box_3.png 1024w" loading=lazy alt=typo3 class=gallery-image data-flex-grow=1006 data-flex-basis=2415px></p><p>注意条件部分，公式应为：$P_{12}=P(S_{t+1}=s_2\vert S_{t}=s_1)$</p><p><strong>3.1</strong></p><p><img src=/collections/drl_task01/image-3.png width=1648 height=728 srcset="/collections/drl_task01/image-3_huf8d91a62bf4b7aeb8410cdaad4ec1d85_205553_480x0_resize_box_3.png 480w, /collections/drl_task01/image-3_huf8d91a62bf4b7aeb8410cdaad4ec1d85_205553_1024x0_resize_box_3.png 1024w" loading=lazy alt=typo4 class=gallery-image data-flex-grow=226 data-flex-basis=543px></p><p>第二行公式加号应为等号：$f(0,0)=f(-1,0)+f(0,-1)$</p></section><footer class=article-footer><section class=article-copyright><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg><span></span></section></footer><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.6/dist/katex.min.css integrity="sha256-J+iAE0sgH8QSz9hpcDxXIftnj65JEZgNhGcgReTTK9s=" crossorigin=anonymous><script src=https://cdn.jsdelivr.net/npm/katex@0.15.6/dist/katex.min.js integrity="sha256-InsNdER1b2xUewP+pKCUJpkhiqwHgqiPXDlIk7GzBu4=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/katex@0.15.6/dist/contrib/auto-render.min.js integrity="sha256-y39Mpg7V3D4lhBX4x6O0bUqTV4pSrfgwEfGKfxkOdgI=" crossorigin=anonymous defer></script><script>window.addEventListener("DOMContentLoaded",()=>{renderMathInElement(document.querySelector(`.article-content`),{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],ignoredClasses:["gist"]})})</script></article><div class=disqus-container><div id=disqus_thread></div><script>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//hugo-theme-stack.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></div><style>.disqus-container{background-color:var(--card-background);border-radius:var(--card-border-radius);box-shadow:var(--shadow-l1);padding:var(--card-padding)}</style><script>window.addEventListener("onColorSchemeChange",e=>{typeof DISQUS=="object"&&DISQUS.reset({reload:!0})})</script><footer class=site-footer><section class=copyright>&copy;
2023 -
2024 Diaspora</section><section class=powerby>Mark's personal repository<br>Built with <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a><br>Theme <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.21.0>Stack</a></b> designed by <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a></section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>